# load external libraries
suppressMessages(library(mboost))



###############################################################################################################
# rpredmod.pred
#   function for getting the prediction for all randomized prediction models
#
# TIME STAMP
#   2009-Jun-16
#
# PARAMETERS
#   rpredmod          multi-class prediction models built upon randimized runs
#   tr.ts             default FALSE, return prediction for TEST data only; if TRUE, both training and test data
#   objannot          annotation file where class labels can be found
#   annot.what        name of the field in objannot that contains class labels
#
# USAGE
#   test = rpredmod.pred (rpredmod=ret, tr.ts=FALSE, objannot=dgep$obs, annot.what="PhenoMCA")
################################################################################################################
rpredmod.pred <- function(rpredmod=NULL,tr.ts=FALSE, objannot=NULL,annot.what="PhenoMCA")
{

#  rpredmod = ret
#  annot.what = "PhenoMCA"
#  objannot = dgep$obs

  obs = rpredmod$obs

  # what prediction to get - 1 for test data; and (0,1) for bot traning and test
  what = 1
  if(tr.ts) what = c(0,1)
  #
  # get the predicted posterior
  #
  tmp = randposterior(rpredmod = rpredmod)
  # sort (by sample names) all posterior matrices generated by each classifier
  tmp = lapply(tmp, FUN=function(x){x[sort(rownames(x)),]})
  if(length(tmp)>1){    # only does this if more than the randomization contains more than one classifiers
                        # for instance, FLU 1vs2, 2vs3, 3vs4 ...
                        # but if we only only at prediction on say 3vs4, this is not necessary to check the sample alignment
    for (i in 1:(length(tmp)-1))
      if(sum(rownames(tmp[[i]])!=rownames(tmp[[i+1]]))>0) stop("posterior prob matrices are not aligned")
  }

  # determine how many randomized prediction were ran
  n.rand = dim(tmp[[1]])[2]

  ret = list()
  ret$prediction = lapply (  tmp
                           , FUN = function(x){
                                                a.ret = list()
                                                for ( i in 1:n.rand) {
                                                  a.ret[[i]] = x[rownames(obs[obs[,i] %in% what,]),i]
                                                  names(a.ret)[i] = colnames(x)[i]
                                                }
                                                return(a.ret)
                                              }
                          )
  ret$clstrue = lapply (  tmp
                           , FUN = function(x){
                                                a.ret = list()
                                                for ( i in 1:n.rand) {
                                                  a.ret[[i]] = getPheno(rownames(obs[obs[,i] %in% what,]),objannot,annot=annot.what)
                                                  names(a.ret)[i] = colnames(x)[i]
                                                }
                                                return(a.ret)
                                              }
                       )
  return(ret)
}

#######################################################################################################
##  clsf::rpredmod.roc
##    plots all pairwise binary ROC curves for a multi-class randomized model
##
##  TIME STAMP
##    2009-Jun-16
##
##  PARAMETERS
##    rpredmod                output from a randomized multi-classifciation model (see clsf::rpredmod)
##    avg                     "none"/"threshold"/"horizontal"/"vertical"
##                            If the performance object describes several curves
##                            (from cross-validation runs or bootstrap evaluations of one particular method),
##                            the curves from each of the runs can be averaged.
##                            Allowed values are none (plot all curves separately),
##                            horizontal (horizontal averaging), vertical (vertical averaging),
##                            and threshold (threshold (=cutoff) averaging).
##                            Note that while threshold averaging is always feasible, vertical and horizontal
##                            averaging are not well-defined if the graph cannot be represented as
##                            a function x->y and y->x, respectively
##    spread.estimate         "boxplot"/"stddev"/"stderror"
##    spread.scale            default 2 gives approximate 95% confidence intervals under normal assumation
##    grid.grain              the dimension of the grid line within plotting box
##    grid.lty                default 3 for dotted line, 2 for dashed line
##    font.size               size of the text font
##    is.add                  whether to add this plot onto an existing plot
##    fpdf                    name of graphics output file, can be either ".pdf" or ".tiff"
##    res.pixel               only necessary for ".tiff" with targeted file for publication
##                            careful on this since file size will be big - for 1024 res.pixel setting, file size 12M
##
##  DEPENDENCIES
##    1) library(ROCR)
##    2) clsf::rpred.roc ()
##    3) clsf::rpredmod.pred ()
##
##  USAGE
##    rpredmod.roc (rpredmod=ret, tr.ts=FALSE, objannot=dgep$obs, class.name="PhenoMCA",is.add=FALSE,fpdf="test.pdf")
##
#######################################################################################################
rpredmod.roc <- function (  rpredmod=NULL,tr.ts=FALSE,objannot=NULL,class.name="PhenoMCA"
                          , avg="threshold",spread.estimate="stddev",spread.scale=2
                          , grid.grain=0.1,grid.lty=3,font.size=1.1
                          , is.add=FALSE, fpdf=NULL, res.pixel=256)
{ # clsf::rpredmod.roc

#  fpdf = "test.pdf"
#  is.add = FALSE

  list.perf = list()

  x = rpredmod.pred (rpredmod=rpredmod, tr.ts=tr.ts, objannot=objannot, annot.what=class.name)

  n.predmod = length(x[[1]])
  names.predmod = names(x$clstrue)

  if(!is.null(fpdf) && right(fpdf,3)=="pdf" ) pdf(file=fpdf, paper="letter", width=10, height=8)
  else if(!is.null(fpdf) && right(fpdf,4)=="tiff") tiff( filename=fpdf, width = 8, height = 11, units = "in", pointsize = 12
                                                 ,compression = "lzw",bg = "white"
                                                 ,res = res.pixel)

  par(mfrow=c(1,1))
  if(!is.add) {
    n.r = ceiling(sqrt(n.predmod))
    n.c = ceiling(n.predmod/n.r)
    par(mfrow=c(n.r,n.c))
  }

  ls.posterior = list()
  ls.clstrue = list()
  # plot ROC for one randomized prediction model at a time
  for (i in 1:n.predmod) {
    # what classes the current classifier was built on. For instance, a=(1,2) or a=(2,3)
    a = as.integer(unlist(strsplit(names(x$clstrue)[i],"vs")))
    # get the indices of the observations belong to the classes the current classifier was built
    b = lapply(x$clstrue[[i]],FUN=function(x){return(which(x %in% a))})

#    ls.posterior = lapply(x$prediction[[i]], FUN=function(x){return(x[b])})
#    ls.clstrue = lapply(x$clstrue[[i]], FUN=function(x){return(x[b])})
    ls.posterior[[i]] = list()
    ls.clstrue[[i]] = list()
    for (j in 1:length(b)) {
      ix = b[[j]]

      ls.posterior[[i]][[j]] = x$prediction[[i]][[j]][ix]
      ls.clstrue[[i]][[j]] = x$clstrue[[i]][[j]][ix]
    }
    names(ls.posterior[[i]]) = names(x$prediction[[i]])
    names(ls.clstrue[[i]]) = names(x$clstrue[[i]])

    if(is.add) list.perf[[i]]=rpred.roc( ls.posterior[[i]],ls.clstrue[[i]]
                          ,avg=avg,spread.estimate=spread.estimate,spread.scale=spread.scale
                          ,grid.grain=grid.grain,grid.lty=grid.lty,font.size=font.size
                          ,main="ROC of All Pairwise Binary Classification"
                          ,is.add=ifelse(i>1,TRUE,FALSE))
    if(!is.add) list.perf[[i]]=rpred.roc( ls.posterior[[i]],ls.clstrue[[i]]
                          ,avg=avg,spread.estimate=spread.estimate,spread.scale=spread.scale
                          ,grid.grain=grid.grain,grid.lty=grid.lty,font.size=font.size
                          ,is.add=FALSE
                          ,main=names.predmod[i])

  }

  names(ls.posterior) = names(x$prediction)
  names(ls.clstrue) = names(x$clstrue)
  names(list.perf) = names(x$clstrue)

  if(!is.null(fpdf)) dev.off()

  return(list.perf)

}


#######################################################################################################
##  clsf::rpred.roc
##    plots one binary classification (out of a multi-class randomized model) ROC curve
##
##  TIME STAMP
##    2009-Jun-16
##
##  PARAMETERS
##    ls.posterior            A vector, matrix, list, or data frame containing the predictions.
##    ls.clstrue              (same dimension as ls.posterior) a vector, matrix, list, or data frame containing the true class labels
##                            >>Supports only binary classification (extensions toward multiclass classification are
##                              scheduled for the next release, however).
##                            >>If there are more than two distinct label symbols, execution stops with an error message.
##                            >>If all predictions use the same two symbols that are used for the labels, categorical predictions are assumed.
##                            >>If there are more than two predicted values, but all numeric, continuous predictions are assumed (i.e. a scoring
##                              classifier).
##                            >>Otherwise, if more than two symbols occur in the predictions, and not all of them are numeric, execution
##                              stops with an error message.
##    avg                     "none"/"threshold"/"horizontal"/"vertical"
##                            If the performance object describes several curves
##                            (from cross-validation runs or bootstrap evaluations of one particular method),
##                            the curves from each of the runs can be averaged.
##                            Allowed values are none (plot all curves separately),
##                            horizontal (horizontal averaging), vertical (vertical averaging),
##                            and threshold (threshold (=cutoff) averaging).
##                            Note that while threshold averaging is always feasible, vertical and horizontal
##                            averaging are not well-defined if the graph cannot be represented as
##                            a function x->y and y->x, respectively
##    spread.estimate         "boxplot"/"stddev"/"stderror"
##    spread.scale            default 2 gives approximate 95% confidence intervals under normal assumation
##    grid.grain              the dimension of the grid line within plotting box
##    grid.lty                line type of grid lines, default 3 for dotted, 2 for dashed line
##    font.size               size of the text font
##    is.add                  whether to add this plot onto an existing plot
##
##  DEPENDENCIES
##    1) library(ROCR)
##
##  USAGE
##    ls.posterior = tmp$posterior
##    ls.clstrue = tmp$clstrue
##    rpredroc(ls.posterior,ls.clstrue,spread.estimate="stddev",spread.scale=2)
#######################################################################################################
rpred.roc <- function (  ls.posterior=NULL,ls.clstrue=NULL
                        , avg="threshold",spread.estimate="stddev",spread.scale=2
                        , grid.grain=0.1,grid.lty=3,font.size=1.1
                        , is.add=F
                        , main="ROC")
{ # clsf::rpredroc

  # plot with ROCR package
  library(ROCR)

  #grain = 0.1
  #font.size = 1.1

  # transform the predicted posterior and true class label into a standardized form for evaluation
  tmp = prediction(ls.posterior,ls.clstrue)
  # evaluate the classifer based on the predicted posterior and true class label
  perf = performance(tmp,"tpr","fpr")

  par(font.axis=2,font.lab=2,cex.axis=font.size,cex.lab=font.size)
  plot(  perf
        ,avg=avg
        ,spread.estimate=spread.estimate
        ,spread.scale=spread.scale
        ,colorize=TRUE, colorize.palette=(rainbow(256,start=0, end=4/6))
        ,clorkey=TRUE, colorkey.relwidth=0.3
        ,downsampling=0
        ,lwd=3
        ,add=is.add
        ,main=main
        ,box.lty=7,box.lwd=2,box.col="black"                # border of box
        ,xaxis.col='black',yaxis.col='black'                # color of axis tick
        ,xaxis.col.axis="black",yaxis.col.axis="black"      # color of axis label
  #      ,yaxis.at=c(0,0.5,0.8,0.85,0.9,1)
        ,yaxis.las=1
        ,xaxis.lwd=2, yaxis.lwd=2                           # axis line width
        ,xaxis.cex.axis=font.size, yaxis.cex.axis=font.size # font size of axis tick labels
        ,xaxis.font=2,yaxis.font=2                          # font style of axis tick labels - bold
        ,cex.lab=font.size
  #      ,cex.lab=1.2
  #      ,cex.axis=1.2
  #      ,cex.main=2
      )

  if(!is.add) {
    # lty=2 dashed; lty=3 dotted
    abline(h=seq(0,1,by=grid.grain),lty=grid.lty,lwd=1)
    abline(v=seq(0,1,by=grid.grain),lty=grid.lty,lwd=1)
  }
  
  return(perf)

}

#
# clsf::randcoefplot
#   function for showing statbility of predictors by ploting top N coefficents from a multi-class boosting with randomization
#
# PARAMETERS
#    ifname              input file name contains the coefficents data frames from multi-class boosting with randomization
#    plot.what           only needed if we want to plot the number of times a predictor (gene) is selected out of all randomization
#    by.what             how the coefficients should be plotted -"howmany"/"absavg"/"stdev"
#    n.coef              number of top coefficients to be plotted
#    stat.summary        what summary statistic to be used;
#                        "smean.cl.normal" >> computes 3 summary variables: the sample mean and lower and upper Gaussian confidence limits
#                                             based on the t-distribution.
#                        "smean.sdl"       >> computes the mean plus or minus a constant (default 2, change using mult=n) times the standard deviation.
#                        "smean.cl.boot"   >> a very fast implementation of the basic nonparametric bootstrap for obtaining confidence limits for the
#                                             population mean without assuming normality.
#                        "smedian.hilow"   >> computes the sample median and a selected pair of outer quantiles having equal tail areas.
#    mult                a constant that stdev should be multiplied if using "smean.sdl"
#
#  USAGE
#    ifname = "r1000m200p900.3vs4.csv"
#    test = randcoefplot(ifname=ifname, by.what="howmany",n.coef=10,stat.summary="smean.sdl",mult=1,geom="crossbar",jitter=T,size=1)
#    test = randcoefplot(ifname=ifname, plot.what="", by.what="howmany",n.coef=10,stat.summary="smean.sdl",mult=1,geom="crossbar",jitter=T,size=1)
#
#  NOTE
#    1) the parameter "mult" does not seem to be working, it should be a bug in ggplot or "stat_summary_dfmX"
#
#  DBG
#   is.add=T; fpdf="test.pdf"; plot.what="howmany";by.what="howmany";n.coef=10;
#   stat.summary=NULL; mult=NULL;geom="errorbar";jitter=TRUE;size=1.5;
#   a.predmod = randcoef(ret)
#   a.predmod.coef = randcoef(ret,csv="rsv.r500m5000s1p651.csv")
#   rpredmod.coefplot(a.predmod.coef,n.coef=10,is.add=T,fpdf="test.pdf")
#
#
# is.add      plot everything onto one page
#
# Known Bugs
#   1) when n.coef is greater than the available, error will be generated
#

rpredmod.coefplot <- function(a.rpredmod = NULL, plot.what="howmany", by.what="howmany", n.coef=10
                              ,stat.summary=NULL, mult=NULL, geom="errorbar", jitter=TRUE, size=1.5
                              , is.add=T, fpdf=NULL)

{ #clsf:: rpredmod.coefplot
  if(is.null(a.rpredmod)) stop("give me the right parameter, you stupid!")

  n.clsf = length(a.rpredmod)
  name.clsf = names(a.rpredmod)

##  par(mfrow=c(1,1))
#  if(is.add) {
#    n.r = ceiling(sqrt(n.clsf))
#    n.c = ceiling(n.clsf/n.r)
##    par(mfrow=c(n.r,n.c))
#    grid.newpage()
#    grid.rect(gp=gpar(fill="white"))
#    pushViewport(viewport(layout=grid.layout(n.r, n.c)))
#  }
#
#
  if (!is.null(fpdf) && right(fpdf,3)=="pdf") pdf(fpdf)
  if(!is.null(fpdf) && right(fpdf,4)=="tiff") tiff( filename=fpdf, width = 7.5, height = 10, units = "in", pointsize = 12
                                                   ,compression = "none",bg = "white"
                                                   ,res = 2048, restoreConsole = TRUE)
  for (i in 1:n.clsf) {

      test = a.rpredmod[[i]]

      ix.runs = grep("^r",colnames(test))
      ix.symb = grep("^symbol$",colnames(test))

      test = test[test$avg!=0,c(ix.symb,ix.runs)]
      ret = test        # keep a copy of coefficents and plotting on test
      # get row and columne index in the subsetted data frame
      ix.runs = grep("^r",colnames(test))
      ix.symb = grep("^symbol$",colnames(test))

      # compute statistics to be used for plotting
      if(by.what=="howmany") {
        test$howmany = apply(test[,ix.runs],1,function(x){return(sum(x!=0))})
        test=top(test,by=~-howmany,n=n.coef)
      }
      if(by.what=="stdev")   {
        test$stdev = apply(test[,ix.runs],1,function(x){return(sqrt(var(x)))})
        test=top(test,by=~+stdev,n=n.coef)
      }
      if(by.what=="absavg")  {
        test$absavg = apply(test[,ix.runs],1,function(x){return(abs(mean(x)))})
        test=top(test,by=~+absavg,n=n.coef)
      }

      #
      # to solve the problem of ggplot v0.8.3 automatically re-arranges the x-axis labels
      #
      test$symbol=factor(1:n.coef,labels=test$symbol)


      if(plot.what=="howmany") {
        theme_set(theme_bw())

        test.plot = qplot(round((howmany/length(ix.runs))*100,0),x=test$symbol,data=test
                          ,geom="bar",xlab="predictors", ylab="Percentage of Randomization Runs"
                          ,colour=I(alpha("black",1/1))
                          ,main=name.clsf[i]              # add the clsfier name as the main title
                          )  + scale_x_discrete(breaks=test$symbol)
        test.plot = test.plot + opts(panel.grid.major=theme_line(size=0.4, linetype="dotted"))
        test.plot = test.plot + opts(   #axis.line=theme_segment(size=1)
                                   panel.border=theme_rect(size=0.7)
                                  , axis.text.y=theme_text(face="bold", size=10, hjust=1)
                                  , axis.text.y=theme_blank()
                                  , axis.text.x=theme_text(face="bold", size=9, hjust=1, vjust=1, angle=45)
                                  , axis.ticks=theme_segment(size=0.6)
                                  , axis.title.x=theme_text(face="bold",size=12)
                                  , axis.title.y=theme_text(face="bold",size=12,angle=90)
                                  , axis.title.y=theme_blank()
                                )
        print(test.plot)
      }

      else {
        #
        # (using reshape) to transform data into right format for plotting
        #
        # dimension of the data to be transformed by reshape
        n.r = nrow(test)
        n.c = ncol(test[,-ix.symb])
      #  test$symbol=factor(1:n.coef,labels=test$symbol)
        # transform
        test.reshape = reshape(test
                              ,v.names="coefval"
                              ,varying=list(colnames(test)[ix.runs])
                              ,idvar="genes"
                              ,ids=rownames(test)
                              ,timevar="runs"
                              ,times=colnames(test)[ix.runs]
                              ,new.row.names=seq(1:(n.r*n.c))
                              ,direction="long"
                              )

        # name of the function for computing statistics
        name.fun = NULL
        if (stat.summary=="smean.cl.normal") name.fun = "mean_cl_normal"
        if (stat.summary=="smean.sdl") name.fun = "mean_sdl"
        if (stat.summary=="smean.cl.boot") name.fun = "mean_cl_boot"
        if (stat.summary=="smedian.hilow") name.fun = "median_hilow"

        # calling ggplot
        if (stat.summary=="smean.sdl" & mult!=2) {  # overload the Hmisc::smean.sdl() function
            print("please make sure you overloaded the function Hmisc::smean.sdl()")
        }

        test.plot = ggplot(test.reshape, aes(x=symbol,coefval), colour=I(alpha("black",1/1)))
        if(jitter) { test.plot = test.plot + geom_jitter(size=0.5, aes(width=0.05))}
        else {test.plot = test.plot + geom_point()}
        test.plot = test.plot + stat_sum_dfmX(fun=name.fun,geom=geom,colour="red",size=size)
        # coord_flip() +
        test.plot = test.plot + labs(x="",y="")
        # test.plot = test.plot + opts(legend.position="none")
        #test.plot = test.plot + scale_x_discrete(breaks=NA)
        test.plot = test.plot + theme_set(theme_bw())
        print(test.plot)
    }
  }
  if (!is.null(fpdf)) dev.off()

  return()

}



#
# clsf::randcoefplot
#   function for showing statbility of predictors by ploting top N coefficents from a multi-class boosting with randomization
#
# PARAMETERS
#    ifname              input file name contains the coefficents data frames from multi-class boosting with randomization
#    plot.what           only needed if we want to plot the number of times a predictor (gene) is selected out of all randomization
#    by.what             how the coefficients should be plotted -"howmany"/"absavg"/"stdev"
#    n.coef              number of top coefficients to be plotted
#    stat.summary        what summary statistic to be used;
#                        "smean.cl.normal" >> computes 3 summary variables: the sample mean and lower and upper Gaussian confidence limits
#                                             based on the t-distribution.
#                        "smean.sdl"       >> computes the mean plus or minus a constant (default 2, change using mult=n) times the standard deviation.
#                        "smean.cl.boot"   >> a very fast implementation of the basic nonparametric bootstrap for obtaining confidence limits for the
#                                             population mean without assuming normality.
#                        "smedian.hilow"   >> computes the sample median and a selected pair of outer quantiles having equal tail areas.
#    mult                a constant that stdev should be multiplied if using "smean.sdl"
#
#  USAGE
#    ifname = "r1000m200p900.3vs4.csv"
#    test = randcoefplot(ifname=ifname, by.what="howmany",n.coef=10,stat.summary="smean.sdl",mult=1,geom="crossbar",jitter=T,size=1)
#    test = randcoefplot(ifname=ifname, plot.what="", by.what="howmany",n.coef=10,stat.summary="smean.sdl",mult=1,geom="crossbar",jitter=T,size=1)
#
#  NOTE
#    1) the parameter "mult" does not seem to be working, it should be a bug in ggplot or "stat_summary_dfmX"
#
#  DBG
#   is.add=T; fpdf="test.pdf"; plot.what="howmany";by.what="howmany";n.coef=10;
#   stat.summary=NULL; mult=NULL;geom="errorbar";jitter=TRUE;size=1.5;
#   a.predmod = randcoef(ret)
#   a.predmod.coef = randcoef(ret,csv="rsv.r500m5000s1p651.csv")
#   rpredmod.coefplot(a.predmod.coef,n.coef=10,is.add=T,fpdf="test.pdf")
#
#
# is.add      plot everything onto one page
#
# Known Bugs
#   1) when n.coef is greater than the available, error will be generated
#
randcoefplot <- function(ifname=NULL,plot.what="howmany",by.what="howmany",n.coef=10,stat.summary=NULL,mult=NULL,geom="errorbar",jitter=TRUE,size=1.5)
{ # cluf::randcoefplot

  if(any(is.null(ifname), is.null(stat.summary))) stop("pass me the right parameters, you stupid idiot!")

  test = read.csv(ifname,header=T,row.names=1,stringsAsFactors=FALSE)
  ix.runs = grep("^r",colnames(test))
  ix.symb = grep("^symbol$",colnames(test))
  # remove those coeffiences that are not selected by boosting produre
  test = test[test$avg!=0,c(ix.symb,ix.runs)]
  ret = test        # keep a copy of coefficents and plotting on test
  # get row and columne index in the subsetted data frame
  ix.runs = grep("^r",colnames(test))
  ix.symb = grep("^symbol$",colnames(test))

  # compute statistics to be used for plotting
  if(by.what=="howmany") {
    test$howmany = apply(test[,ix.runs],1,function(x){return(sum(x!=0))})
    test=top(test,by=~-howmany,n=n.coef)
  }
  if(by.what=="stdev")   {
    test$stdev = apply(test[,ix.runs],1,function(x){return(sqrt(var(x)))})
    test=top(test,by=~+stdev,n=n.coef)
  }
  if(by.what=="absavg")  {
    test$absavg = apply(test[,ix.runs],1,function(x){return(abs(mean(x)))})
    test=top(test,by=~+absavg,n=n.coef)
  }

  #
  # to solve the problem of ggplot v0.8.3 automatically re-arranges the x-axis labels
  #
  test$symbol=factor(1:n.coef,labels=test$symbol)

  if(plot.what=="howmany") {
    theme_set(theme_bw())

    test.plot = qplot(round((howmany/length(ix.runs))*100,0),x=test$symbol,data=test
                      ,geom="bar",xlab="predictors", ylab="Percentage of Randomization Runs"
                      ,colour=I(alpha("black",1/1))
                      )  + scale_x_discrete(breaks=test$symbol)
    print(test.plot)
    return(ret)
  }

  #
  # (using reshape) to transform data into right format for plotting
  #
  # dimension of the data to be transformed by reshape
  n.r = nrow(test)
  n.c = ncol(test[,-ix.symb])
#  test$symbol=factor(1:n.coef,labels=test$symbol)
  # transform
  test.reshape = reshape(test
                        ,v.names="coefval"
                        ,varying=list(colnames(test)[ix.runs])
                        ,idvar="genes"
                        ,ids=rownames(test)
                        ,timevar="runs"
                        ,times=colnames(test)[ix.runs]
                        ,new.row.names=seq(1:(n.r*n.c))
                        ,direction="long"
                        )

  # name of the function for computing statistics
  name.fun = NULL
  if (stat.summary=="smean.cl.normal") name.fun = "mean_cl_normal"
  if (stat.summary=="smean.sdl") name.fun = "mean_sdl"
  if (stat.summary=="smean.cl.boot") name.fun = "mean_cl_boot"
  if (stat.summary=="smedian.hilow") name.fun = "median_hilow"

  # calling ggplot
  if (stat.summary=="smean.sdl" & mult!=2) {  # overload the Hmisc::smean.sdl() function
      print("please make sure you overloaded the function Hmisc::smean.sdl()")
  }

  test.plot = ggplot(test.reshape, aes(x=symbol,coefval), colour=I(alpha("black",1/1)))
  if(jitter) { test.plot = test.plot + geom_jitter(size=0.5, aes(width=0.05))}
  else {test.plot = test.plot + geom_point()}
  test.plot = test.plot + stat_sum_dfmX(fun=name.fun,geom=geom,colour="red",size=size)
  # coord_flip() +
  test.plot = test.plot + labs(x="",y="")
  # test.plot = test.plot + opts(legend.position="none")
  #test.plot = test.plot + scale_x_discrete(breaks=NA)
  test.plot = test.plot + theme_set(theme_bw())
  print(test.plot)
  return(ret)
}

#################################################################################################################
#
# clsf::randposterior
#   get the posterior probability matrices for all pair-wise comparison in a randomized multi-class boosting test
#
# TIME STAMP
#   2009-Jun-14
#
# PARAMETERS
#   rpredmod        a list holding the results of randomizd multi-class boosting run {clsf::rpredmod}
#   csv             name of a ".csv" file to be exported in the current working directory
#
# USAGE
# x = randcoef(ret,csv="r1000m200p900.csv")
#
#################################################################################################################
randposterior <- function(rpredmod = NULL,csv=NULL)
{ # clsf::randcoef
  ix.clsfiers = grep("vs",names(rpredmod))
  ret = rpredmod[ix.clsfiers]
  print(ix.clsfiers)
  ret = lapply(ret,FUN=function(x){return(x$posterior)})

  if(!is.null(csv))
    for (i in 1:length(ret))
      write.csv(ret[[i]],paste(gsub(".csv","",csv),".",names(ret)[i],".csv",sep=""))

  return(ret)
}



#
# clsf::randcoef
#   get the coeffient matrices for all pair-wise comparison in a randomized multi-class boosting test
#
# TIME STAMP
#   2009-Jun-14
#
# PARAMETERS
#   rpredmod        a list holding the results of randomizd multi-class boosting run {clsf::rpredmod}
#   csv             name of a ".csv" file to be exported in the current working directory
#
# USAGE
# x = randcoef(ret,csv="r1000m200p900.csv")
#
randcoef <- function(rpredmod = NULL,csv=NULL)
{ # clsf::randcoef
  ix.clsfiers = grep("vs",names(rpredmod))
  ret = rpredmod[ix.clsfiers]
  print(ix.clsfiers)
  ret = lapply(ret,FUN=function(x){return(x$coef)})

  if(!is.null(csv))
    for (i in 1:length(ret))
      write.csv(ret[[i]],paste(gsub(".csv","",csv),".",names(ret)[i],".csv",sep=""))

  return(ret)
}


#
# clsf::ovoPredict
#   predict with one-vs-one boosting scheme
#
# TIME STAMP
#   2009-06-09
#
# PARAMETERS
#   ovomboost:  an object (list) with each element being an object of mboost
#   what:       what type of prediction - "response" or "lp" (half of loglike)
#
# RETURN VALUES
#   ret:        a list of prediction given by each mboost fit
#
# DEPENDENCIES
#
#
#ovoPredict <- function (ovomboost=NULL,newdata=NULL, what="response") {
#  return(ret.posterior = lapply(ovomboost, function(x) { print(str(x)); return (predict(x, newdata,what)) }))
#}
#


################################################################################################################
## This following implementation was commented on 2009-Jul-09
## with new implementation of boosting with fix list of predictors for different classifiers
## Refer to ovomb (as part of this new implementation)
##
## =====================================================================================
## clsf::rpredmod
##
## Multi-classes boosting
##    randomized prediction model results for stability study of
##    predictors and multiclass boosting classfier (based on mboost)
##
## TIME STAMP
##   2009-Jun-09
##
## PARAMETERS
##    ds:         data frame contains class labels and data
##    ix.cls:      index of column which holds class labels (default = 1 for first column)
##    mctype:     method to be used for multiclass comparison - "ovomb" (one-versus-one) ....
##    risktype:   inbag / oobag / - see mboost:risktype
##    n.rand:     total number of randomization runs
##    prop.tr:    proportion of samples (in each class) to be used for training
##    n.miter:    number of boosting iterations per randomized run
##    link:       link functions to be used - see mboost::family
##    model:      type of boosting model (see mboost::glmboost / gamboost / blacktree)
##    risktype:   empirical risk to be computed as inbag / oobag / none
##    shrinkage:  amount of shrinkage for baselearner
##    objannot:   an object of annotation of features
##
## DEPENDENCIES
##    1) 'class'              is asummed to be the name of the column holding the class labels
##    2) phdlib::getFeatures
##
## RETURN VALUE
##   lmcmboost: a list of 'mboost' object
##              where # of elements is # of one-versus-all comparison
##
## DESIGN
##    notations
##      n - number of observations;
##      p - number of genes;
##      q - number of boosting classifiers (depending on how many randomized run are)
##
##
##    $1vs2
##      $aic          1  x  q
##      $offset       1  x  q         # holding the offset value for each classifier
##      $coef         p  x  (q + 3)   # one extra column holding the average of coefficients
##                                    # one for gene symbol and one for gene description
##      $posterior    n  x  q
##      $response     n  x  q         # predicted response
##      $fit          n  x  q         # fitted value (half of log likelihood ratio)
##    $1vs3
##      ...
##    $1vs4
##      ...
##    $2vs3
##      ...
##    $3vs4
##      ...
##    $prediction     n  x (q+2)
##      $clstrue                      # true class labels
##      $prederr                      # sample-wise number of misclassification (out of q random runs)
##    $obs            n  x  q         # selection (random) of samples for training and test
##
##  NOTE
##    0) currently, only one-versus-one comparison is implemented
##    1) to avoid hitting memory limit, will not save the actual boosting classifier
##    2) the posteriors are not directly comparable between classifiers in ONE-VS-ONE scheme
##    3) for each pair-wise comparison, response/posterior/fit will be NA for training samples that do not belong to that class pairs
##
##  USAGE
#     system.time ( { test.rpredmod = rpredmod ( ds=test, ix.cls=NULL
#                                              ,mctype="ovo"
#                                              ,n.rand=1,prop.tr=0.7
#                                              ,n.miter=1,link="binomial",model="glm"
#                                              ,risktype="inbag",shrinkage=0.05
#                                              ,objannot=dgep$features)
#                   }
#                 )
#     test.rpedmod.aic = sapply(ret[grep('vs',names(ret))], FUN=function(x){return(x$aic)})    # to get the aic values
#
#  DBG
#   ds=NULL; ix.cls=NULL; mctype="ovo";n.rand=2; prop.tr=0.7;n.miter=5;
#   link="binomial";model="glm";risktype="inbag";shrinkage=0.05;objannot=NULL
################################################################################################################
#rpredmod <- function (ds=NULL, ix.cls=NULL
#                      ,mctype="ovo"
#                      ,n.rand=2, prop.tr=0.7
#                      ,n.miter=5,link="binomial",model="glm"
#                      ,risktype="inbag",shrinkage=0.05
#                      ,objannot=NULL
#                      ,verbose=FALSE)
#{ # clsf::rpredmod
#
#  #DBG
#  #n.rand=2              # number of randomization runs
#  #n.miter=2             # number of boosting iterations / randomized run
#  #prop.tr = 0.7
#  #ds = test
#
#  if (mctype=="ovo") {
#
#    # initiate the data struct for holding prediction results
#    if(is.null(ix.cls))     ix.cls = grep("class",colnames(ds))     # index of column holding class labels
#    else colnames(ds)[ix.cls] = "class"
#
#    cls = ds[,ix.cls]
#    all.class = NULL
#    if(class(cls)=="factor")  all.class = sort(as.integer(levels(cls)))
#    else if(class(cls)=="numeric")  all.class = sort(levels(cls))
#    else stop("class labels are neither numeric nor factor, dummie!")
#
#    n.cls = length(unique(ds[,ix.cls]))     # number of classes
#    n.clsf = choose(n.cls,2)                # number of classifiers (one for each two classes)
#    p = dim(ds)[2] -1                       # number of predictor genes minus one (class label)
#    n = dim(ds)[1]                          # number of observations
#    p.names = colnames(ds)[-ix.cls]         # names of predictors (genes)
#    n.names = rownames(ds)                  # names of observations
#    r.names = paste("r",1:n.rand,sep="")    # names of randomization runs
#
#    ret = list()
#    for (i in 1:n.clsf){
#        ret[[i]] = list(aic=NULL, offset=NULL
#                       ,coef=data.frame(matrix(NA,nrow=p,ncol=n.rand,dimnames=list(p.names,r.names)))
#                       ,posterior=data.frame(matrix(NA,nrow=n,ncol=n.rand,dimnames=list(n.names,r.names)))
#                       ,response=data.frame(matrix(NA,nrow=n,ncol=n.rand,dimnames=list(n.names,r.names)))
#                       ,fit=data.frame(matrix(NA,nrow=n,ncol=n.rand,dimnames=list(n.names,r.names)))
#                       )
#    }
#    tmp = combn(all.class,2); names(ret) = paste(tmp[1,], "vs",tmp[2,],sep=""); rm(tmp);
#    clsf.names = names(ret)                  # names of classifiers (same for every randomization run)
#
#    # to hold aggregated (majority vote) prediction (from all classifiers / each randomization run)
#    ret$prediction = data.frame(matrix(NA,nrow=n,ncol=n.rand,dimnames=list(n.names,r.names)))
#
#    #
#    # create and store a randomized sample matrix into ret$obs
#    #
#    ret$obs = randcls(ds=cbind(ids=n.names, class=cls), by=ids~class,prop=prop.tr,nrand=n.rand)
#
#    for (ix.rand in 1:n.rand) {
#
#      if(verbose) cat(paste(">>> ", ix.rand, "-th randomized multiclass boosting...\n", sep=""))
#
#      spl.tr = rownames(ret$obs)[ret$obs[,ix.rand]==0]
#      spl.ts = rownames(ret$obs)[ret$obs[,ix.rand]==1]
#
#      test.tr = ds[spl.tr,];   test.ts = ds[spl.ts,]
#      rm(spl.tr,spl.ts)
#
#      #
#      # calling the ovoboosting procedure
#      #
#      test.tr.mcb = ovomb(ds=test.tr,ixcls=ix.cls
#                          ,miter=n.miter,model=model,link=link
#                          ,risktype=risktype,shrinkage=shrinkage)
#      x.test = mcmboostSet(test.tr.mcb)
#      # make sure the return value is valid
#      if (length(x.test)!=2) stop("function ovomb returns something unexpected")
#
#      #
#      # store the results into the data structure (ret)
#      #
#
#      # store AIC
#      for (i in 1:n.clsf) {
#        ret[[ clsf.names[i] ]]$aic =  c(ret[[ clsf.names[i] ]]$aic, mstop(x.test$aic[[ clsf.names[i] ]]))
#      }
#
#      # store offset value
#      for (i in 1:n.clsf) {
#        ret[[ clsf.names[i] ]]$offset =  c(ret[[ clsf.names[i] ]]$offset, x.test$clsf[[ clsf.names[i] ]]$offset)
#      }
#
#      # store the coefficients
#      for (i in 1:n.clsf) {
#        a.clsf.coef = coef(x.test$clsf[[ clsf.names[i] ]])[-1]        # get the coefficients w/o intercepts for one classifier at a time
#        tmp.names = gsub("`","",names(a.clsf.coef))
#        ret[[ clsf.names[i] ]]$coef[tmp.names, ix.rand] =  a.clsf.coef
#      }
#
#      # store the posterior
#      tmp = mcmboostPosterior(omcmboost=x.test$clsfier)
#      for (i in 1:n.clsf) {
#        ret[[ clsf.names[i] ]]$posterior[names(tmp[[ clsf.names[i] ]]),ix.rand] = tmp[[ clsf.names[i] ]]
#      }
#      rm (tmp)
#
#      # store the fitted value (1/2 of log likelihood ratio of each classifier
#      # also store the posterior based on the fitted values 1/(1+exp(-2*fitted)
#      tr.clstrue = data.frame(class=test.tr$class,row.names=rownames(test.tr))
#      test.tr.pred = ovombPred(x.test$clsfier,testdata=NULL,clstrue=tr.clstrue,what="lp")
#      ts.clstrue = data.frame(class=test.ts$class,row.names=rownames(test.ts))
#      test.ts.pred = ovombPred(x.test$clsfier,testdata=test.ts,clstrue=ts.clstrue,what="lp")
#
#      for (i in 1:n.clsf) {
#        tmp.tr = test.tr.pred[,clsf.names[i]];  names(tmp.tr) = rownames(test.tr.pred);  tmp.tr = tmp.tr[!is.na(tmp.tr)]  # training
#        tmp.ts = test.ts.pred[,clsf.names[i]];  names(tmp.ts) = rownames(test.ts.pred);  tmp.ts = tmp.ts[!is.na(tmp.ts)]  # test
#        tmp = c(tmp.tr, tmp.ts)
#        ret[[ clsf.names[i] ]]$fit[names(tmp),ix.rand] = tmp
#        ret[[ clsf.names[i] ]]$posterior[names(tmp),ix.rand] = 1/(1+exp(-2*as.numeric(tmp)))
#        rm(tmp)
#      }
#
#      # store the response of each classifier
#      tr.clstrue = data.frame(class=test.tr$class,row.names=rownames(test.tr))
#      test.tr.pred = ovombPred(x.test$clsfier,testdata=NULL,clstrue=tr.clstrue,what="response")
#      ts.clstrue = data.frame(class=test.ts$class,row.names=rownames(test.ts))
#      test.ts.pred = ovombPred(x.test$clsfier,testdata=test.ts,clstrue=ts.clstrue,what="response")
#      for (i in 1:n.clsf) {
#        tmp.tr = test.tr.pred[,clsf.names[i]];  names(tmp.tr) = rownames(test.tr.pred);  tmp.tr = tmp.tr[!is.na(tmp.tr)]  # training
#        tmp.ts = test.ts.pred[,clsf.names[i]];  names(tmp.ts) = rownames(test.ts.pred);  tmp.ts = tmp.ts[!is.na(tmp.ts)]  # test
#        tmp = c(tmp.tr, tmp.ts)
#        ret[[ clsf.names[i] ]]$response[names(tmp),ix.rand] = tmp
#        rm(tmp)
#      }
#
#      # store the aggregated prediction (majority vote) of one randomization run
#      ret$prediction[c(rownames(test.tr.pred),rownames(test.ts.pred)), ix.rand] = c(test.tr.pred$clspred, test.ts.pred$clspred)
#      rm(tr.clstrue,test.tr.pred,ts.clstrue,test.ts.pred)
#
#      rm(test.tr.mcb,x.test); gc();
#    }
#
#    # sample-wise summary statistics on prediction error rate
#    ret$prediction$clstrue = ds$class
#    tmp=apply(ret$prediction, MARGIN=1
#          , FUN=function(x){ix.cls=which(attr(x,"names")=="clstrue"); x=as.numeric(x); error=sum(x[-ix.cls]!=x[ix.cls]); return(error)})
#    ret$prediction[attr(tmp,"names"),"prederr"] = tmp
#    rm(tmp)
#
#    for (i in grep("vs",names(ret))) {
#      tmp = apply(ret[[i]]$coef, MARGIN=1, FUN=function(x){mean(x)})
#      ret[[i]]$coef[attr(tmp,"names"),"avg"] = tmp
#      rm(tmp)
#      if(!is.null(objannot)) {
#        tmp = ret[[i]]$coef;
#        rownames(tmp)=gsub("^X","",rownames(tmp))
#        tmp$symbol = getFeatures(idfeatures=rownames(tmp), annot="Symbol",objannot=objannot)
#        tmp$gene = getFeatures(idfeatures=rownames(tmp), annot="Description",objannot=objannot)
#        ret[[i]]$coef=tmp
#        rm(tmp)
#      }
#    }
#
#  return (ret)
#
#  } # end of ovomb section
#
#  #return(ret)
#
#}

## clsf::rpredmod
##
## Multi-classes boosting
##    randomized prediction model results for stability study of
##    predictors and multiclass boosting classfier (based on mboost)
##
## TIME STAMP
##   2009-Jun-09
##
## PARAMETERS
##    ds:         data frame contains class labels and data
##    ix.cls:      index of column which holds class labels (default = 1 for first column)
##    mctype:     method to be used for multiclass comparison - "ovomb" (one-versus-one) ....
##    risktype:   inbag / oobag / - see mboost:risktype
##    n.rand:     total number of randomization runs
##    prop.tr:    proportion of samples (in each class) to be used for training
##    n.miter:    number of boosting iterations per randomized run
##    link:       link functions to be used - see mboost::family
##    model:      type of boosting model (see mboost::glmboost / gamboost / blacktree)
##    risktype:   empirical risk to be computed as inbag / oobag / none
##    shrinkage:  amount of shrinkage for baselearner
##    objannot:   an object of annotation of features
##    predictors: only necessary if different set of predictor to be used for each classifier;
##                a data frame contains predictors to be used in each comparison
##
##
## DEPENDENCIES
##    1) 'class'              is asummed to be the name of the column holding the class labels
##    2) phdlib::getFeatures
##    3) predictors:    if specified, it has to be a dataframe wich each column containing predictors for one comparison, e.g.,
##                         1vs2       2vs3
##                         100_at     100_at
##                         5365_at    64135_at
##
##
## RETURN VALUE
##   lmcmboost: a list of 'mboost' object
##              where # of elements is # of one-versus-all comparison
##
## DESIGN
##    notations
##      n - number of observations;
##      p - number of genes;
##      q - number of boosting classifiers (depending on how many randomized run are)
##
##
##    $1vs2
##      $aic          1  x  q
##      $offset       1  x  q         # holding the offset value for each classifier
##      $coef         p  x  (q + 3)   # one extra column holding the average of coefficients
##                                    # one for gene symbol and one for gene description
##      $posterior    n  x  q
##      $response     n  x  q         # predicted response
##      $fit          n  x  q         # fitted value (half of log likelihood ratio)
##    $1vs3
##      ...
##    $1vs4
##      ...
##    $2vs3
##      ...
##    $3vs4
##      ...
##    $prediction     n  x (q+2)
##      $clstrue                      # true class labels
##      $prederr                      # sample-wise number of misclassification (out of q random runs)
##    $obs            n  x  q         # selection (random) of samples for training and test
##
##  NOTE
##    0) currently, only one-versus-one comparison is implemented
##    1) to avoid hitting memory limit, will not save the actual boosting classifier
##    2) the posteriors are not directly comparable between classifiers in ONE-VS-ONE scheme
##    3) for each pair-wise comparison, response/posterior/fit will be NA for training samples that do not belong to that class pairs
##
##  USAGE
#     system.time ( { test.rpredmod = rpredmod ( ds=test, ix.cls=NULL
#                                              ,mctype="ovo"
#                                              ,n.rand=1,prop.tr=0.7
#                                              ,n.miter=1,link="binomial",model="glm"
#                                              ,risktype="inbag",shrinkage=0.05
#                                              ,objannot=dgep$features)
#                   }
#                 )
#     test.rpedmod.aic = sapply(ret[grep('vs',names(ret))], FUN=function(x){return(x$aic)})    # to get the aic values
#
#  DBG
#   ds=NULL; ix.cls=NULL; mctype="ovo";n.rand=2; prop.tr=0.7;n.miter=5;
#   link="binomial";model="glm";risktype="inbag";shrinkage=0.05;objannot=NULL
################################################################################################################
rpredmod <- function (ds=NULL, ix.cls=NULL
                      ,mctype="ovo"
                      ,n.rand=2, prop.tr=0.7
                      ,n.miter=5,link="binomial",model="glm"
                      ,risktype="inbag",shrinkage=0.05
                      ,objannot=NULL
                      ,verbose=FALSE
                      ,predictors=NULL)
{ # clsf::rpredmod

  #DBG
  #n.rand=2              # number of randomization runs
  #n.miter=2             # number of boosting iterations / randomized run
  #prop.tr = 0.7
  #ds = test

  if (mctype=="ovo") {

    # initiate the data struct for holding prediction results
    if(is.null(ix.cls))     ix.cls = grep("class",colnames(ds))     # index of column holding class labels
    else colnames(ds)[ix.cls] = "class"

    cls = ds[,ix.cls]
    all.class = NULL
    if(class(cls)=="factor")  all.class = sort(as.integer(levels(cls)))
    else if(class(cls)=="numeric")  all.class = sort(levels(cls))
    else stop("class labels are neither numeric nor factor, dummie!")

    n.cls = length(unique(ds[,ix.cls]))     # number of classes
    n.clsf = choose(n.cls,2)                # number of classifiers (one for each two classes)
    p = dim(ds)[2] -1                       # number of predictor genes minus one (class label)
    n = dim(ds)[1]                          # number of observations
    p.names = colnames(ds)[-ix.cls]         # names of predictors (genes)
    n.names = rownames(ds)                  # names of observations
    r.names = paste("r",1:n.rand,sep="")    # names of randomization runs

    ret = list()
    for (i in 1:n.clsf){
        ret[[i]] = list(aic=NULL, offset=NULL
                       ,coef=data.frame(matrix(NA,nrow=p,ncol=n.rand,dimnames=list(p.names,r.names)))
                       ,posterior=data.frame(matrix(NA,nrow=n,ncol=n.rand,dimnames=list(n.names,r.names)))
                       ,response=data.frame(matrix(NA,nrow=n,ncol=n.rand,dimnames=list(n.names,r.names)))
                       ,fit=data.frame(matrix(NA,nrow=n,ncol=n.rand,dimnames=list(n.names,r.names)))
                       )
    }
    tmp = combn(all.class,2); names(ret) = paste(tmp[1,], "vs",tmp[2,],sep=""); rm(tmp);
    clsf.names = names(ret)                  # names of classifiers (same for every randomization run)

    # to hold aggregated (majority vote) prediction (from all classifiers / each randomization run)
    ret$prediction = data.frame(matrix(NA,nrow=n,ncol=n.rand,dimnames=list(n.names,r.names)))

    #
    # create and store a randomized sample matrix into ret$obs
    #
    ret$obs = randcls(ds=cbind(ids=n.names, class=cls), by=ids~class,prop=prop.tr,nrand=n.rand)

    for (ix.rand in 1:n.rand) {

      if(verbose) cat(paste(">>> ", ix.rand, "-th randomized multiclass boosting...\n", sep=""))

      spl.tr = rownames(ret$obs)[ret$obs[,ix.rand]==0]
      spl.ts = rownames(ret$obs)[ret$obs[,ix.rand]==1]

      test.tr = ds[spl.tr,];   test.ts = ds[spl.ts,]
      rm(spl.tr,spl.ts)

      #
      # calling the ovoboosting procedure
      #
      if (!is.null(predictors)) {
              test.tr.mcb = ovomb(ds=test.tr,ixcls=ix.cls
                          ,miter=n.miter,model=model,link=link
                          ,risktype=risktype,shrinkage=shrinkage,predictors=predictors)
      }
      else {
        test.tr.mcb = ovomb(ds=test.tr,ixcls=ix.cls
                            ,miter=n.miter,model=model,link=link
                            ,risktype=risktype,shrinkage=shrinkage)
      }
      x.test = mcmboostSet(test.tr.mcb)
      # make sure the return value is valid
      if (length(x.test)!=2) stop("function ovomb returns something unexpected")

      #
      # store the results into the data structure (ret)
      #

      # store AIC
      for (i in 1:n.clsf) {
        ret[[ clsf.names[i] ]]$aic =  c(ret[[ clsf.names[i] ]]$aic, mstop(x.test$aic[[ clsf.names[i] ]]))
      }

      # store offset value
      for (i in 1:n.clsf) {
        ret[[ clsf.names[i] ]]$offset =  c(ret[[ clsf.names[i] ]]$offset, x.test$clsf[[ clsf.names[i] ]]$offset)
      }

      # store the coefficients
      for (i in 1:n.clsf) {
        a.clsf.coef = coef(x.test$clsf[[ clsf.names[i] ]])[-1]        # get the coefficients w/o intercepts for one classifier at a time
        tmp.names = gsub("`","",names(a.clsf.coef))
        ret[[ clsf.names[i] ]]$coef[tmp.names, ix.rand] =  a.clsf.coef
      }

      # store the posterior
      tmp = mcmboostPosterior(omcmboost=x.test$clsfier)
      for (i in 1:n.clsf) {
        ret[[ clsf.names[i] ]]$posterior[names(tmp[[ clsf.names[i] ]]),ix.rand] = tmp[[ clsf.names[i] ]]
      }
      rm (tmp)

      # store the fitted value (1/2 of log likelihood ratio of each classifier
      # also store the posterior based on the fitted values 1/(1+exp(-2*fitted)
      tr.clstrue = data.frame(class=test.tr$class,row.names=rownames(test.tr))
      test.tr.pred = ovombPred(x.test$clsfier,testdata=NULL,clstrue=tr.clstrue,what="lp")
      ts.clstrue = data.frame(class=test.ts$class,row.names=rownames(test.ts))
      test.ts.pred = ovombPred(x.test$clsfier,testdata=test.ts,clstrue=ts.clstrue,what="lp")

      for (i in 1:n.clsf) {
        tmp.tr = test.tr.pred[,clsf.names[i]];  names(tmp.tr) = rownames(test.tr.pred);  tmp.tr = tmp.tr[!is.na(tmp.tr)]  # training
        tmp.ts = test.ts.pred[,clsf.names[i]];  names(tmp.ts) = rownames(test.ts.pred);  tmp.ts = tmp.ts[!is.na(tmp.ts)]  # test
        tmp = c(tmp.tr, tmp.ts)
        ret[[ clsf.names[i] ]]$fit[names(tmp),ix.rand] = tmp
        ret[[ clsf.names[i] ]]$posterior[names(tmp),ix.rand] = 1/(1+exp(-2*as.numeric(tmp)))
        rm(tmp)
      }

      # store the response of each classifier
      tr.clstrue = data.frame(class=test.tr$class,row.names=rownames(test.tr))
      test.tr.pred = ovombPred(x.test$clsfier,testdata=NULL,clstrue=tr.clstrue,what="response")
      ts.clstrue = data.frame(class=test.ts$class,row.names=rownames(test.ts))
      test.ts.pred = ovombPred(x.test$clsfier,testdata=test.ts,clstrue=ts.clstrue,what="response")
      for (i in 1:n.clsf) {
        tmp.tr = test.tr.pred[,clsf.names[i]];  names(tmp.tr) = rownames(test.tr.pred);  tmp.tr = tmp.tr[!is.na(tmp.tr)]  # training
        tmp.ts = test.ts.pred[,clsf.names[i]];  names(tmp.ts) = rownames(test.ts.pred);  tmp.ts = tmp.ts[!is.na(tmp.ts)]  # test
        tmp = c(tmp.tr, tmp.ts)
        ret[[ clsf.names[i] ]]$response[names(tmp),ix.rand] = tmp
        rm(tmp)
      }

      # store the aggregated prediction (majority vote) of one randomization run
      ret$prediction[c(rownames(test.tr.pred),rownames(test.ts.pred)), ix.rand] = c(test.tr.pred$clspred, test.ts.pred$clspred)
      rm(tr.clstrue,test.tr.pred,ts.clstrue,test.ts.pred)

      rm(test.tr.mcb,x.test); gc();
    }

    # sample-wise summary statistics on prediction error rate
    ret$prediction$clstrue = ds$class
    tmp=apply(ret$prediction, MARGIN=1
          , FUN=function(x){ix.cls=which(attr(x,"names")=="clstrue"); x=as.numeric(x); error=sum(x[-ix.cls]!=x[ix.cls]); return(error)})
    ret$prediction[attr(tmp,"names"),"prederr"] = tmp
    rm(tmp)

    for (i in grep("vs",names(ret))) {
      tmp = apply(ret[[i]]$coef, MARGIN=1, FUN=function(x){mean(x)})
      ret[[i]]$coef[attr(tmp,"names"),"avg"] = tmp
      rm(tmp)
      if(!is.null(objannot)) {
        tmp = ret[[i]]$coef;
        rownames(tmp)=gsub("^X","",rownames(tmp))
        tmp$symbol = getFeatures(idfeatures=rownames(tmp), annot="Symbol",objannot=objannot)
        tmp$gene = getFeatures(idfeatures=rownames(tmp), annot="Description",objannot=objannot)
        ret[[i]]$coef=tmp
        rm(tmp)
      }
    }

  return (ret)

  } # end of ovomb section

  #return(ret)

}


#
# clsf::randcls
#   create a data frame by randomizing a given sample list
#
# TIME STAMP
#   2009-Jun-09
#
# PARAMETERS
#   ds:       a data frame contains observations and their class labels
#   by:       a one-sided formula (see USAGE)
#   prop:     proportion of each class should be
#   nrand:    how many randomization should be run, ie., number of columns in the returned matrix
#
# RETURN VALUE
#   ret:      a matrix - OBSERVATIONS (row) x RANDOMIZATIONS (column)
#
# USAGE
# test = randcls(ds=tmp,by=IDShort~PhenoMCA+Subject,prop=0.5,nrand=6)
# test = randcls(ds=tmp,by=IDShort~PhenoMCA,prop=0.7,nrand=7)
#
# NOTE
#   1) potentially extend the function to make it beyond two-class situation
#      this should be very easy by making prop = c(0.5, 0.3, 0.2 ) etc
#
randcls <- function (ds=NULL,by=~.,prop=0.5,nrand=100){ # clsf::randcls

  ret = NULL

  if(is.matrix(ds)) ds = data.frame(ds)

  if(by[[1]] != "~")                stop("Hey stupid, argument 'by' must be a one-sided formula.")
  if(length(by)==2 & by[[2]]!=".")  stop("Hey stupid, argument 'by' must be a one-sided formula.")

  ids = StrTrim(by[[2]])

  # Make the formula into character and remove spaces
  formc <- as.character(by[3])
  formc <- gsub(" ", "", formc)
  # If the first character is not + or -, add +
  if(!is.element(substring(formc, 1, 1), c("+", "-")))      formc <- paste("+", formc, sep = "")
  # Extract the variables from the formula
  vars <- unlist(strsplit(formc, "[\\+]"))
  vars <- vars[vars != ""]                              # Remove any extra "" terms

  ds$group = apply(data.frame(ds[,vars]), MARGIN=1, FUN=paste, sep="",collapse=" ")

  ret = matrix(NA,nrow=dim(ds)[1],ncol=nrand, dimnames=list(ds[,ids], paste("sp",seq(1:nrand),sep="")))

  group.xtab = table(ds$group)
  n.group = length(group.xtab)
  howmany = round(group.xtab * prop,0)

  for (i in 1:nrand) {

    # per group sampling
    ix.spls = NULL
    for (j in 1:n.group) {
      whatspl = names(group.xtab)[j]
      one.sample = sample(which(ds$group==whatspl),howmany[j])
      ix.spls = c(ix.spls, one.sample)
    }

    ret[ix.spls,i] = 0
    ret[is.na(ret[,i]), i] = 1
  }

  if(is.null(ret)) stop("clsf::randcls() failed.")

  return (ret)
}



##
## clsf::maj.vote
##    implements majority voting mechanism (a utility function for classification)
##
## PARAMETERS
##    avec    -   numeric (with NA possible); a vector of class labels
##
## RETURN VALUE
##    -1      -   if a tie;   numeric class label   -   if majority is identified
##
maj.vote <- function (avec=NULL) {
  cnt = table(avec)
  ix.max = as.integer(attr(which(cnt==max(cnt)),"names"))
  return(ifelse(length(ix.max)>1,-1,ix.max))
}


##
## note
##    this max posterior approach does NOT work
##
## clsf::maxposterior.vote
##    vote class label by max posterior probability) (a utility function for classification)
##
## PARAMETERS
##    avec    -   numeric (with NA possible); a vector of class labels
##
## RETURN VALUE
##    ret     -   numeric class label
##
#maxposterior.vote <- function (avec=NULL) {
#  ret = NULL
#  p_half = 0.5
#  posterior = 1/(exp(1)^(-2*avec))
#  #print(posterior)
#  ab.deviation = abs(posterior-p_half)
#  ix.max = which.max(ab.deviation)
#
#  # assuming class labels always in format of "[:alpha:]1vs2"
#  clslabels = as.numeric(gsub( "[[:alpha:]]"
#                               ,""
#                               ,unlist(strsplit(attr(avec,"names")[ix.max],split="vs"))
#                              )
#                        )
#
#  if(posterior[ix.max]>p_half) ret=clslabels[2]
#  else ret = clslabels[1]
#
#  return(ret)
#}


##
## clsf::ovombPred
##
## Multi-classes boosting
##    one-vs-one classification wrapper for "predict" function
##
## PARAMETERS
##    lsmcmboost:     a list of 'mboost' objects for multi-classes boosting
##    testdata:       a new dataset (same format as training) on which prediction to be made
##                    >>> if NULL, training data will be used instead
##    what:           what type of prediction to be made {response, posterior}
##    ds:             data frame contains class labels and data
##    ds:             data frame contains class labels and data
##    ixcls:          index of column which holds class labels (default = 1 for first column)
##    miter:          iteration of boosting
##    link:           link functions to be used - see mboost::family
##    model:          type of boosting model (see mboost::glmboost / gamboost / blacktree)
##    risk:           empirical risk to be computed as inbag / oobag / none
##
## RETURN VALUE
##   lmcmboost: a list of 'mboost' object
##              where # of elements is # of one-versus-all comparison
##
## DEPENDENCIES
##   clsf::maj.vote
##
## DBG
##
##
## USAGE
##
##
################################################################################################################
ovombPred <- function (lsmcmboost=NULL, testdata=NULL, clstrue=NULL, what="response") {

  test.tr.mcb = lsmcmboost
  n.mboost = length(test.tr.mcb)
  pred = list()
  sapl = NULL
  for (i in 1:n.mboost) {
    if(is.null(testdata)){
      pred[[i]]=predict(test.tr.mcb[[i]],type=what)
      names(pred[[i]]) = attr(test.tr.mcb[[i]]$data$x,'dimnames')[[1]]
    }
    else {
      pred[[i]]=predict(test.tr.mcb[[i]],newdata=testdata,type=what)
      names(pred[[i]]) = rownames(testdata)
    }

    sapl = union(sapl,names(pred[[i]]))
  }

  # colname of the pred table using names of classifier
  names(pred) = names(test.tr.mcb)

  ret = data.frame(matrix(NA,nrow=length(sapl),ncol=n.mboost))
  rownames(ret) = sapl
  colnames(ret) = names(pred)

  # populate the table
  for (i in 1:length(pred)) {
    ret[match(names(pred[[i]]), rownames(ret)), i] = as.character(pred[[i]])
  }

  ret = data.frame(ret)

  if(what=="response")  ret[,'clspred']=apply(ret,1,maj.vote)
  else if(what=="lp")        ret[,'clspred']=apply(ret,1,function(x){return(NA)})              # return "lp" value only, without prediction
  else                  ret[,"clspred"]=apply(ret,1,maxposterior.vote)
  if(!is.null(clstrue)) ret[,'clstrue']=clstrue[match(rownames(ret),rownames(clstrue)),'class']

  # remove that extra X symbole in each column name
  colnames(ret) = gsub("X","",colnames(ret))

  return (ret)
}

##########################################################################################
## clsf::ovomb
##
## Multi-classes boosting
##    implements a ONE-vs-ONE (ovo) boosting for multi-classes labels
##
## PARAMETERS
##    ds:         data frame contains class labels and data
##    ixcls:      index of column which holds class labels (default = 1 for first column)
##    miter:      iteration of boosting
##    link:       link functions to be used - see mboost::family
##    model:      type of boosting model (see mboost::glmboost / gamboost / blacktree)
##    risk:       empirical risk to be computed as inbag / oobag / none
##    predictors: only necessary if different set of predictor to be used for each classifier;
##                a data frame contains predictors to be used in each comparison
##
## RETURN VALUE
##   lmcmboost: a list of 'mboost' object
##              where # of elements is # of one-versus-all comparison
##  DBG
##
##  USAGE
##    1) normal randomized boosting runs
##
##
#test.ovombfix = ovomb(ds=test, ixcls=21, miter=500
#                        , model='glm', link='binomial', sample.weights=NULL
#                        , risktype='inbag',shrinkage=0.5, weaklearner='btree'
#                        , predictors=paste("X",list.what$features,sep=""))
##########################################################################################
ovomb <- function ( ds=NULL,ixcls=1,miter=500
                      ,model='glm',link='binomial'
                      ,sample.weights=NULL
                      ,risktype='oobag',shrinkage=0.5
                      ,weaklearner='btree', predictors = NULL)
{ # clsf::ovomb

  cls = ds[,ixcls]
  if(class(cls)=="factor")  all.class = sort(as.integer(levels(cls)))
  else if(class(cls)=="numeric")  all.class = sort(levels(cls))
  else stop("class labels are neither numeric nor factor, dummie!")

  colnames(ds)[ixcls]='class'

  # if more than 50% of columns starting with letter "X",
  # means R automatic adding an X in front of probesets
  if(length(grep("X",colnames(ds))) > (ncol(ds)/2)) prefix="X"
  else prefix=""

  n.class=length(all.class)
  lmcmboost = list()
  n.mboost = 0

  #
  # boosting K_choose_2 comparison
  #
  for (i in 1:(n.class-1))
    for (j in (i+1):n.class) {
      if(is.null(predictors)) {ds.tmp = ds[ds[,ixcls] %in% c(all.class[i],all.class[j]), ]}
      else { # we need to subset the predictor list to be used in prediction accordingly
            tmp.p = predictors[,paste("X", all.class[i],"vs",all.class[j],sep="")]
            tmp.p = tmp.p[tmp.p!=""]
            ds.tmp = ds[  ds[,ixcls] %in% c(all.class[i],all.class[j])
                        , c('class', paste(prefix, tmp.p, sep=""))
                       ]
            rm(tmp.p)
      }
      ds.tmp = drop.levels(ds.tmp)            # drop unused levels
#      ds.tmp = ds.tmp[drop=TRUE]              # equivalent way of dropping unused levels
#      print(dim(ds.tmp))
#      print(table(ds.tmp[,ixcls]))
#      return(0)
      n.mboost = n.mboost + 1

      if(model=='glm'){
        # training classifier with boosting on generalized linear model
        # with Binomial as link (loss) function (logit)
        # or AdaExp as link (loss) function (Adaboost)
        if(link=='binomial') lmcmboost[[n.mboost]]=glmboost( class~., data=ds.tmp
                                                     ,weights=sample.weights
                                                     ,family = Binomial()
                                                     ,control = boost_control(mstop=miter,risk=risktype,nu=shrinkage,center=TRUE))
        if(link=='adaexp') lmcmboost[[n.mboost]] = glmboost( class~., data=ds.tmp
                                                     ,family = AdaExp()
                                                     ,control = boost_control(mstop = miter,risk=risktype))
        names(lmcmboost)[n.mboost] = paste(all.class[i],"vs",all.class[j],sep="")
      }

      if(model=='gam'){
        # fitting generalized additive model
        # with Binomial as link (loss) function (logit)
        # or AdaExp as link (loss) function (Adaboost)
        if(link=='binomial') lmcmboost[[n.mboost]] = gamboost( class~., data=ds
                                                       ,baselearner=weaklearner,family = Binomial()
                                                       ,control = boost_control(mstop = miter,risk=risktype))
        if(link=='adaexp') lmcmboost[[n.mboost]] = gamboost( class~., data=ds
                                                     ,baselearner=weaklearner,family = AdaExp()
                                                     ,control = boost_control(mstop = miter,risk=risktype))
        names(lmcmboost)[n.mboost] = paste(all.class[i],"vs",all.class[j],sep="")
      }

  }

  return(lmcmboost)
}



##########################################################################################
## This following implementation was commented on 2009-Jul-09
## with new implementation of boosting with fix list of predictors for different classifiers
## Refer to rpredmod (as part of this new implementation)
##
## =====================================================================================
## clsf::ovomb
##
## Multi-classes boosting
##    implements a ONE-vs-ONE (ovo) boosting for multi-classes labels
##
## PARAMETERS
##    ds:       data frame contains class labels and data
##    ixcls:    index of column which holds class labels (default = 1 for first column)
##    miter:    iteration of boosting
##    link:     link functions to be used - see mboost::family
##    model:    type of boosting model (see mboost::glmboost / gamboost / blacktree)
##    risk:     empirical risk to be computed as inbag / oobag / none
##
## RETURN VALUE
##   lmcmboost: a list of 'mboost' object
##              where # of elements is # of one-versus-all comparison
##########################################################################################
#ovomb <- function ( ds=NULL,ixcls=1,miter=500
#                      ,model='glm',link='binomial'
#                      ,sample.weights=NULL
#                      ,risktype='oobag',shrinkage=0.5
#                      ,weaklearner='btree')
#{ # clsf::ovomb
#
#  cls = ds[,ixcls]
#  if(class(cls)=="factor")  all.class = sort(as.integer(levels(cls)))
#  else if(class(cls)=="numeric")  all.class = sort(levels(cls))
#  else stop("class labels are neither numeric nor factor, dummie!")
#
#  n.class=length(all.class)
#  lmcmboost = list()
#  n.mboost = 0
#
#  #
#  # boosting K_choose_2 comparison
#  #
#  for (i in 1:(n.class-1))
#    for (j in (i+1):n.class) {
#      ds.tmp = ds[ds[,ixcls] %in% c(all.class[i],all.class[j]),]
#      ds.tmp = drop.levels(ds.tmp)            # drop unused levels
##      ds.tmp = ds.tmp[drop=TRUE]              # equivalent way of dropping unused levels
##      print(dim(ds.tmp))
##      print(table(ds.tmp[,ixcls]))
##      return(0)
#      n.mboost = n.mboost + 1
#
#      if(model=='glm'){
#        # training classifier with boosting on generalized linear model
#        # with Binomial as link (loss) function (logit)
#        # or AdaExp as link (loss) function (Adaboost)
#        if(link=='binomial') lmcmboost[[n.mboost]]=glmboost( class~., data=ds.tmp
#                                                     ,weights=sample.weights
#                                                     ,family = Binomial()
#                                                     ,control = boost_control(mstop=miter,risk=risktype,nu=shrinkage,center=TRUE))
#        if(link=='adaexp') lmcmboost[[n.mboost]] = glmboost( class~., data=ds.tmp
#                                                     ,family = AdaExp()
#                                                     ,control = boost_control(mstop = miter,risk=risktype))
#        names(lmcmboost)[n.mboost] = paste(all.class[i],"vs",all.class[j],sep="")
#      }
#
#      if(model=='gam'){
#        # fitting generalized additive model
#        # with Binomial as link (loss) function (logit)
#        # or AdaExp as link (loss) function (Adaboost)
#        if(link=='binomial') lmcmboost[[n.mboost]] = gamboost( class~., data=ds
#                                                       ,baselearner=weaklearner,family = Binomial()
#                                                       ,control = boost_control(mstop = miter,risk=risktype))
#        if(link=='adaexp') lmcmboost[[n.mboost]] = gamboost( class~., data=ds
#                                                     ,baselearner=weaklearner,family = AdaExp()
#                                                     ,control = boost_control(mstop = miter,risk=risktype))
#        names(lmcmboost)[n.mboost] = paste(all.class[i],"vs",all.class[j],sep="")
#      }
#
#  }
#
#  return(lmcmboost)
#}


#
# func::mcmboostXcnt
#   to obtain the number of unique predictors selected in each iteration
#
# TIME STAMP
#     2009-04-08
#
# PARAMETERS
#   omcmboost:  an object (list) with each elments being an object of mboost
#
# RETURN VALUES
#   ret.xcnt: number of predictors used in each boosted classifier
#
# DEPENDENCIES
#   func::coef{mboost}
#
mcmboostXcnt <- function (omcmboost=NULL) {

  n.mboost = length(omcmboost)

  ret.xcnt = list()
  for (ix in 1:n.mboost) {
    amboost = omcmboost[[ix]]
    amboost
    n.iter = dim(amboost$ensemble)[1]
    n.predictor = NULL
    for (i in 1:n.iter) {tmp=expression(1:i);n.predictor = c(n.predictor,length(unique(amboost$ensemble[eval(tmp),1])))}
    ret.xcnt[[ix]] = n.predictor
  }
  return (ret.xcnt)
}



#
# func::mcmboostPosterior
#   compute the posterior probability of observations (samples/chips)
#
# TIME STAMP
#     2009-06-09
#
# PARAMETERS
#   omcmboost:  an object (list) with each element being an object of mboost
#
# RETURN VALUES
#   ret.posterior: a list of poseterior probabilities given by each mboost fit
#
# DEPENDENCIES
#

mcmboostPosterior <- function (omcmboost=NULL) {
  return(ret.posterior = lapply(omcmboost, function(x) {1/(1+exp(-2*x$fit))}))
}

#
# func::mcmboostCoef
#   find which coefficents (for each individual mboost object) are greater than 0
#
# TIME STAMP
#     2009-03-22
#
# PARAMETERS
#   omcmboost:  an object (list) with each elments being an object of mboost
#
# RETURN VALUES
#   m.coef:     a list of coefficients that are greater than zero for each mboost object
#
# DEPENDENCIES
#   func::coef{mboost}
#

mcmboostCoef <- function (omcmboost=NULL) {

  n.mboost = length(omcmboost)

  m.coef = list()
  for (i in 1:n.mboost) m.coef[[i]] = which(abs(coef(omcmboost[[i]]))>0)

  return (m.coef)

}

#
# func::mcmboostSet
#   set boosting iteration according to AIC model selection
#
# TIME STAMP
#     2009-03-21
#
# PARAMETERS
#   omcmboost:  an object (list) with each elments being an object of mboost
#
# RETURN VALUES
#   omcmboost:  original 'omcmboost' object with elements being replaced by AIC selected mboost object
#
# DEPENDENCIES
#   func::mcmboostAIC
#
mcmboostSet <- function (omcmboost=NULL) {

  n.mboost = length(omcmboost)
  oaic = mcmboostAIC (omcmboost,showplot=FALSE)
  for (i in 1:n.mboost) omcmboost[[i]] = (omcmboost[[i]])[mstop(oaic[[i]])]

  return (list(aic=oaic, clsfier=omcmboost))

}


#
# func::mcmboostAIC
#   select model for multi-boosting object using AIC criteria (minimizing AIC)
#
# TIME STAMP
#     2009-03-22
#
# PARAMETERS
#   omcmboost:  an object (list) with each elments being an object of mboost
#   showplot:   whether model selection plot should be shown; default = FALSE
#
# RETURN VALUES
#   retAIC:     a vector of integer number of AIC selected
#

mcmboostAIC <- function (omcmboost=NULL,showplot=FALSE) {

  n.mboost = length(omcmboost)      # number of boosting classifiers in the slot

#  retAIC = list()
#
#  for (i in 1:n.mboost) {
#    tmp = NULL
#    tmp = omcmboost[[i]]
#    retAIC[[i]] = AIC(tmp,method='classical')
#  }

  retAIC = lapply(omcmboost,function(x) {AIC(x,method='classical')})

  if (showplot) {
    #n.row=ceiling(sqrt(n.mboost))
    #n.col=n.row
    #par(mfrow=c(n.row,n.col))
    i = 1
    plot(retAIC[[i]],col=i)
    for (i in 2:n.mboost) {
      par(new=T)
      plot(retAIC[[i]],col=i,yaxt='n')
    }

    legend.x = ceiling(0.6*length(attr(retAIC[[1]],label='AIC')))
    legend.y = ceiling(0.7*max(sapply(1:n.mboost, function(ix) {max(attr(retAIC[[ix]],'AIC'))})))
    legend( x=legend.x,y=legend.y
            ,text.col=c(1:n.mboost),legend=paste(rep('class',n.mboost),seq(1:n.mboost),sep=' ')
            ,lty='solid',col=c(1:n.mboost)
            ,bty="o", box.col = par("fg")
          )
  }

  return(retAIC)

}



#
# mcmboostPred
#   multi-class generalization of mboost::predict function
#
# PARAMETERS
#   omcmboost:  an object (list) with each elments being an object of mboost
#   type:       a string indicator of what type of prediction results to be extracted
#
# RETURN VALUES
#   retPreds:   case-dependent upon parameter type
#
mcmboostPred <- function (omcmboost=NULL,dsnew=NULL,ixcls=1, type='response') {

  n.mboost = length(omcmboost)      # number of boosting classifiers in the slot

  # list holding the results
  y = list()
#  y$lpratio = NULL
#  y$y = NULL
#  y$response = NULL

  ret = NULL

  # for a new dataset that was not in original "mcmboost" fitting
  if(!is.null(dsnew)) {

    # design matrix for multiple tests
    mtxdesign = clsDesign(dsnew[,ixcls])

    for (i in 1:n.mboost) {
      dsnew[,ixcls] = as.factor(mtxdesign[,i])
      y$lpratio = cbind(y$lpratio,predict(omcmboost[[i]],newdata=dsnew,type='lp'))
    }

    if(type=='lp') ret = y$lpratio
    if(type=='response') ret = sapply(1:nrow(y$lpratio), function(ixrow) {which.max(y$lpratio[ixrow,])} )

    return (ret)
    # return(invisible(y))
  }

  n.class =  n.mboost               # the two are the SAME in one-veruse-all comparison
  n.observation = length((omcmboost[[1]])$fit)  # sufficient to check only first boost classifier

  retPreds = NULL

  for (i in 1:n.mboost) {
    y$lpratio = cbind(y$lpratio,(omcmboost[[i]])$fit)           # 1/2 * log_2(p(Y=1|X=x)/p(Y=-1|X=x))
    y$y = cbind(y$y, (omcmboost[[i]])$data$y)                   # true class label
#    attr(y$y,'dimnames')[[1]]=attr(y$lpratio,'dimnames')[[1]]
    y$response = cbind(y$response, (omcmboost[[i]])$response)   # response labels
#    attr(y$response,'dimnames')[[1]]=attr(y$lpratio,'dimnames')[[1]]
  }

  attr(y$y,'dimnames')=attr(y$lpratio,'dimnames')
  attr(y$response,'dimnames')=attr(y$lpratio,'dimnames')

  if (type=='response') retPreds = sapply( 1:n.observation,function(ix) {which.max(y$lpratio[ix,])} )
  if (type=='lp') retPreds = y$lpratio             # return only half log2 class probability ratio
  if (type=='both') retPreds = y            # all info returned

  return(retPreds)

}

#test.ts.pred = mcmboostPred (omcmboost=test.tr.mcb,dsnew=test.ts,ixcls=779,type='lp')
##
## Multi-classes boosting
##    implements a one-versus-all boosting for multi-classes labels
##
## PARAMETERS
##    ds:       data frame contains class labels and data
##    ixcls:    index of column which holds class labels (default = 1 for first column)
##    miter:    iteration of boosting
##    link:     link functions to be used - see mboost::family
##    model:    type of boosting model (see mboost::glmboost / gamboost / blacktree)
##    risk:     empirical risk to be computed as inbag / oobag / none
##
## RETURN VALUE
##   lmcmboost: a list of 'mboost' object
##              where # of elements is # of one-versus-all comparison
##
##
mcmboost <- function ( ds=NULL,ixcls=1,miter=500
                      ,model='glm',link='binomial'
                      ,sample.weights=NULL
                      ,risktype='oobag',shrinkage=0.5
                      ,weaklearner='btree')
{
  n.class=length(unique(ds[,ixcls]))
  lmcmboost = list()
  # design matrix for multiple tests
  mtxdesign = clsDesign(ds[,ixcls])
  if(n.class != dim(mtxdesign)[2]) stop("# of class doesn't match the design matrix")
  if(model=='glm'){
    # training classifier with boosting on generalized linear model
    # with Binomial as link (loss) function (logit)
    for (i in 1:n.class) {
      ds$class = as.factor(mtxdesign[,i])
      if(link=='binomial') lmcmboost[[i]]=glmboost( class~., data=ds
                                                   ,family = Binomial()
                                                   ,weights = sample.weights
                                                   ,control = boost_control(mstop=miter,risk=risktype,nu=shrinkage,center=TRUE))
      if(link=='adaexp') lmcmboost[[i]] = glmboost( class~., data=ds
                                                   ,family = AdaExp()
                                                   ,control = boost_control(mstop = miter,risk=risktype))
    }
  }

  if(model=='gam'){
    #fitting generalized additive model
    # with Binomial as link (loss) function (logit)
    for (i in 1:n.class) {
      ds$class = as.factor(mtxdesign[,i])
      if(link=='binomial') lmcmboost[[i]] = gamboost(class~., data=ds, baselearner=weaklearner,family = Binomial(), control = boost_control(mstop = miter,risk=risktype))
      if(link=='adaexp') lmcmboost[[i]] = gamboost(class~., data=ds, baselearner=weaklearner,family = AdaExp(), control = boost_control(mstop = miter,risk=risktype))
    }
  }

  return(lmcmboost)
}


##
## clsDesign
##   create a design matrix with a vector of class labels
##
## PARAMETERS
##   avec:    a vector holding class labels
##
## RETURN VALUE
##   mtxdesign: design matrix in the form of data frame of factors
##
##
## NOTE
##    binary coding: 1 for target class (one) versus 2 for others
##
clsDesign <- function (avec=NULL){
  #dbg:
  #avec = c(rep(1,3),rep(2,3),rep(3,3))
  if(is.null(avec)) stop("need input of a vector of class labels")
  mtxdesign = NULL
  n.class = sort(unique(avec))
  n.obs = length(avec)
  if (n.obs<=2) stop("Design matrix is created only for classes with >2 levels")
  for(i in 1:length(n.class)){
    tmp=rep(NA,n.obs)
    ix = which(avec==n.class[i])
#    tmp[ix]= cstr(n.class[i])
#    tmp[-ix]=cstr(n.class[-i])
    tmp[ix]= 1
    tmp[-ix]=-1
    mtxdesign=cbind(mtxdesign,tmp)
  }
  colnames(mtxdesign) = paste("cls",n.class,sep="")
  mtxdesign = data.frame(mtxdesign,stringsAsFactors=TRUE)
  return(mtxdesign)
}


#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
#
# Relevant exemplary code
#
#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

#
#GM multiclass ada boosting
#
#library(ada)
#n=1200; p=10; K=3;
#set.seed(100)
#x = matrix(rnorm(n*p),ncol=p)
#indtrain = sample(1:n, 200, FALSE)
#indtest = setdiff(1:n,indtrain)
#val=qchisq(c(.33,.66),10)
#su = apply(x^2,1,sum)
#Iy=cbind(as.numeric(su<=val[1]), as.numeric(val[1]<su & su<=val[2]), as.numeric(su>val[2]))
#y=apply(Iy,1,which.max)
#test = data.frame(y=y[indtest],x[indtest,])
#Fs=list()
#for(i in 1:K)
#  Fs[[i]] = ada(y~.,data=data.frame(y=Iy[indtrain,i],x[indtrain,]),
#		iter=250,test.x=test,
#		test.y=Iy[indtest,i])$model$F[[2]]
#preds = sapply(1:1000,function(i)which.max(c(Fs[[1]][i],Fs[[2]][i],Fs[[3]][i])))
#tab = table(y[indtest],preds)
#for(i in 1:K){
#	cat("In class error rate for class",i,": ",
#	round(1-tab[i,i]/sum(tab[i,]),3),"\n")
#}
#

#
# mboost
#
#library(mboost)
#x = Westbc$assay
#y = Westbc$pheno$nodal.y
#x = t(x-rowMeans(x))
#westglm = glmboost(x,y,family=Binomial(),control=boost_control(mstop=5000))
#coef(westglm)>0
#sum(which(coef(westglm)>0))
#sum(coef(westglm)>0)
#coef(westglm)>0
#test = westglm
#str(test)
#n.iter = dim(test$ensemble)[1]
#n.iter
#n.predictor = NULL
#for (i in 1:n.iter) {a=expression(1:i);n.predictor = c(n.predictor,length(unique(test$ensemble[eval(a),1])))}
##qplot(n.predictor,x=seq(1:500),geom='line')
#qplot(test$risk,x=n.predictor,geom='line')
#test.aic=(AIC(test,'classical'))
#qplot(attr(test.aic,"AIC"), x=n.predictor,geom='line')
cat(">>>>>>> Classification library has been successfully loaded!\n")